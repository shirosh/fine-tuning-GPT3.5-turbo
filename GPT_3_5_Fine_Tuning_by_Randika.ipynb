{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirosh/fine-tuning-GPT3.5-turbo/blob/main/GPT_3_5_Fine_Tuning_by_Randika.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuned GPT-3.5\n",
        "By Randika (https://x.com/randikasilva)\n",
        "\n",
        "The goal of this notebook is to experiment with a new way to make it very easy to build a task-specific model for your use-case.\n",
        "\n",
        "\n",
        "Select a temperature (high=creative, low=precise), and the number of training examples to generate to train the model. From there, just run all the cells.\n",
        "\n",
        "You can change the model you want to fine-tune by changing `model_name` in the `Define Hyperparameters` cell."
      ],
      "metadata": {
        "id": "wM8MRkf8Dr94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data generation step"
      ],
      "metadata": {
        "id": "Way3_PuPpIuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Then, choose the temperature (between 0 and 1) to use when generating data. Lower values are great for precise tasks, like writing code, whereas larger values are better for creative tasks, like writing stories.\n",
        "\n",
        "Finally, choose how many examples you want to generate. The more you generate, a) the longer it takes and b) the more expensive data generation will be. But generally, more examples will lead to a higher-quality model. 100 is usually the minimum to start."
      ],
      "metadata": {
        "id": "lY-3DvlIpVSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A model that takes customer feedbacks regarding sold Mobile phone accessories and responds with the sentiment of feeedback by mentioning Positive or Negative.\"\n",
        "temperature = .4\n",
        "number_of_examples = 50"
      ],
      "metadata": {
        "id": "R7WKZyxtpUPS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7d9e3e9f-e5cf-4043-c0ec-2a5d830d8f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this to generate the dataset."
      ],
      "metadata": {
        "id": "1snNou5PrIci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai\n",
        "!pip install tenacity"
      ],
      "metadata": {
        "id": "zuL2UaqlsmBD",
        "outputId": "ae0195d7-3ffa-41e6-95f6-bb08fa2fff26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import random\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  api_key='YOUR API KEY HERE',  # this is also the default, it can be omitted\n",
        ")\n",
        "\n",
        "\n",
        "N_RETRIES = 3\n",
        "\n",
        "@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))\n",
        "def generate_example(prompt, prev_examples, temperature=.5):\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if len(prev_examples) > 0:\n",
        "        if len(prev_examples) > 8:\n",
        "            prev_examples = random.sample(prev_examples, 8)\n",
        "        for example in prev_examples:\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": example\n",
        "            })\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=1000,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Generate examples\n",
        "prev_examples = []\n",
        "for i in range(number_of_examples):\n",
        "    print(f'Generating example {i}')\n",
        "    example = generate_example(prompt, prev_examples, temperature)\n",
        "    #print(example)\n",
        "    prev_examples.append(example)\n",
        "\n",
        "print(prev_examples)"
      ],
      "metadata": {
        "id": "Rdsd82ngpHCG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b74a6b0-2a66-403c-dac6-bf33a3aca20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating example 0\n",
            "Generating example 1\n",
            "Generating example 2\n",
            "Generating example 3\n",
            "Generating example 4\n",
            "Generating example 5\n",
            "Generating example 6\n",
            "Generating example 7\n",
            "Generating example 8\n",
            "Generating example 9\n",
            "Generating example 10\n",
            "Generating example 11\n",
            "Generating example 12\n",
            "Generating example 13\n",
            "Generating example 14\n",
            "Generating example 15\n",
            "Generating example 16\n",
            "Generating example 17\n",
            "Generating example 18\n",
            "Generating example 19\n",
            "Generating example 20\n",
            "Generating example 21\n",
            "Generating example 22\n",
            "Generating example 23\n",
            "Generating example 24\n",
            "Generating example 25\n",
            "Generating example 26\n",
            "Generating example 27\n",
            "Generating example 28\n",
            "Generating example 29\n",
            "Generating example 30\n",
            "Generating example 31\n",
            "Generating example 32\n",
            "Generating example 33\n",
            "Generating example 34\n",
            "Generating example 35\n",
            "Generating example 36\n",
            "Generating example 37\n",
            "Generating example 38\n",
            "Generating example 39\n",
            "Generating example 40\n",
            "Generating example 41\n",
            "Generating example 42\n",
            "Generating example 43\n",
            "Generating example 44\n",
            "Generating example 45\n",
            "Generating example 46\n",
            "Generating example 47\n",
            "Generating example 48\n",
            "Generating example 49\n",
            "['prompt\\n-----------\\nA customer writes: \"The phone case I bought is really durable and stylish. I love it!\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The screen protector I purchased is not fitting properly and keeps coming off. I am very disappointed with the quality.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The wireless charger I bought is very convenient and charges my phone quickly. I am impressed with the performance.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone stand I ordered arrived broken and the quality is very poor. I am extremely dissatisfied with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The portable power bank I purchased is sleek and lightweight. It\\'s perfect for traveling and has been a lifesaver for me.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The car phone holder I bought is not sturdy and keeps dropping my phone. It\\'s really frustrating and not worth the money.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The Bluetooth earphones I purchased have excellent sound quality and the battery life is impressive. I am very satisfied with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone grip I bought is not as comfortable as I expected and it feels flimsy. I regret this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone wallet case I purchased is stylish and functional. It\\'s great to have all my essentials in one place. I am very happy with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The wireless charger I purchased is efficient and charges my phone quickly. I am very pleased with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone stand I purchased is very versatile and holds my phone securely in both portrait and landscape mode. I am extremely happy with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The car phone mount I bought is not sturdy and keeps falling off the dashboard. I am very frustrated with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The selfie stick I purchased is not working properly and the Bluetooth connection is unreliable. I am disappointed with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The portable power bank I purchased is sleek and powerful. It\\'s a lifesaver when I\\'m on the go. I highly recommend it!\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The car phone mount I purchased is not sturdy and keeps falling off the dashboard. I am frustrated with the poor quality of this product.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The screen protector I bought is not as durable as advertised. It got scratched within a week. I am disappointed with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone stand I purchased is not stable and keeps tipping over. I am disappointed with the quality of this product.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The wireless charger I purchased is efficient and charges my phone quickly. I am very satisfied with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone case I bought is stylish and offers great protection. I am very happy with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone charger I bought is not working properly and the cable is fraying. I am very frustrated with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone case I purchased is not only stylish but also provides great protection. I am very satisfied with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The car mount I purchased is very sturdy and holds my phone securely while driving. I am very satisfied with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The portable power bank I purchased is compact and has a long battery life. I am very pleased with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone stand I ordered arrived damaged and the build quality is very poor. I am extremely disappointed with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The screen protector I bought is flimsy and easily gets scratched. I am disappointed with the quality of this product.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The screen protector I purchased is not durable at all. It started peeling off within a week of use. I am very disappointed with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The screen protector I purchased is flimsy and easily gets scratched. I am very disappointed with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The power bank I purchased has a sleek design and provides fast charging. I am very satisfied with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The car mount I ordered arrived damaged and the suction cup doesn\\'t hold well. I am extremely dissatisfied with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The Bluetooth earphones I bought have a great sound quality and are very comfortable to wear. I am extremely happy with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone case I bought is not as durable as I expected, and the color is fading. I am disappointed with the quality of this product.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The power bank I bought is compact and charges my phone quickly. I am very satisfied with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone case I purchased is not as durable as advertised. It broke after a minor drop. I am very disappointed with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone case I bought is stylish and provides excellent protection. I am very happy with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The wireless earbuds I bought have excellent sound quality and the battery life is impressive. I am extremely satisfied with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The screen protector I purchased is not as durable as advertised. It got scratched easily and now my screen is damaged. I am very disappointed with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone charger I bought is flimsy and doesn\\'t charge my phone properly. I am very disappointed with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The portable power bank I bought is compact and provides fast charging. I am very satisfied with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The wireless earbuds I purchased are comfortable to wear and have excellent sound quality. I am very satisfied with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The car phone holder I bought is very sturdy and the suction cup holds it firmly in place. I am very satisfied with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The screen protector I bought is easy to apply and provides great protection. I am very satisfied with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The wireless charger I purchased is sleek and charges my phone quickly. I am very happy with this product and would recommend it to others.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The portable charger I purchased is compact and charges my phone quickly. I am very satisfied with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The car phone holder I bought is flimsy and doesn\\'t securely hold my phone in place. I am very disappointed with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone case I bought is really stylish and provides great protection. I am very happy with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The power bank I bought is compact and charges my phone quickly. I am impressed with the performance and portability.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone case I bought is stylish and provides excellent protection for my phone. I am extremely satisfied with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The phone case I bought is very stylish and provides excellent protection. I am extremely happy with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The screen protector I bought is easy to install and provides excellent protection. I am very satisfied with this product.\"\\n-----------\\n\\nresponse\\n-----------\\nPositive\\n-----------', 'prompt\\n-----------\\nA customer writes: \"The car phone mount I bought is not as sturdy as I expected. It keeps slipping and doesn\\'t hold my phone securely. I am quite disappointed with this purchase.\"\\n-----------\\n\\nresponse\\n-----------\\nNegative\\n-----------']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to generate a system message."
      ],
      "metadata": {
        "id": "KC6iJzXjugJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_system_message(prompt):\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt.strip(),\n",
        "          }\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=500,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "system_message = generate_system_message(prompt)\n",
        "\n",
        "print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')"
      ],
      "metadata": {
        "id": "xMcfhW6Guh2E",
        "outputId": "c92a9a28-9543-4776-d3dd-1408fadb20e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The system message is: `Given a customer feedback about a sold mobile phone accessory, you will determine the sentiment as Positive or Negative.`. Feel free to re-run this cell if you want a better result.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's put our examples into a dataframe and turn them into a final pair of datasets."
      ],
      "metadata": {
        "id": "G6BqZ-hjseBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize lists to store prompts and responses\n",
        "prompts = []\n",
        "responses = []\n",
        "\n",
        "# Parse out prompts and responses from examples\n",
        "for example in prev_examples:\n",
        "  try:\n",
        "    split_example = example.split('-----------')\n",
        "    prompts.append(split_example[1].strip())\n",
        "    responses.append(split_example[3].strip())\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'prompt': prompts,\n",
        "    'response': responses\n",
        "})\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print('There are ' + str(len(df)) + ' successfully-generated examples.')\n",
        "\n",
        "# Initialize list to store training examples\n",
        "training_examples = []\n",
        "\n",
        "# Create training examples in the format required for GPT-3.5 fine-tuning\n",
        "for index, row in df.iterrows():\n",
        "    training_example = {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_message.strip()},\n",
        "            {\"role\": \"user\", \"content\": row['prompt']},\n",
        "            {\"role\": \"assistant\", \"content\": row['response']}\n",
        "        ]\n",
        "    }\n",
        "    training_examples.append(training_example)\n",
        "\n",
        "# Save training examples to a .jsonl file\n",
        "with open('training_examples.jsonl', 'w') as f:\n",
        "    for example in training_examples:\n",
        "        f.write(json.dumps(example) + '\\n')"
      ],
      "metadata": {
        "id": "7CEdkYeRsdmB",
        "outputId": "101213b3-2a13-4986-f6b8-35c7059f1d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 50 successfully-generated examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload the file to OpenAI"
      ],
      "metadata": {
        "id": "KWTY6qVgXD_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = client.files.create(\n",
        "  file=open(\"/content/training_examples.jsonl\", \"rb\"),\n",
        "  purpose='fine-tune'\n",
        ").id"
      ],
      "metadata": {
        "id": "4LjEUrI9XDgT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a44b7cfa-fb00-46b1-bf44-d2c95ae3a79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model! You may need to wait a few minutes before running the next cell to allow for the file to process on OpenAI's servers."
      ],
      "metadata": {
        "id": "HmYRIq8dW9IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job = client.fine_tuning.jobs.create(training_file=file_id, model=\"gpt-3.5-turbo-1106\")\n",
        "\n",
        "job_id = job.id"
      ],
      "metadata": {
        "id": "rdEyXmkoW80I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "bae1151e-48a6-467d-c7fb-3e7ddf3e489f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, just wait until the fine-tuning run is done, and you'll have a ready-to-use model!\n",
        "\n",
        "Run this cell every 20 minutes or so -- eventually, you'll see a message \"New fine-tuned model created: ft:gpt-3.5-turbo-0613:xxxxxxxxxxxx\"\n",
        "\n",
        "Once you see that message, you can go to the OpenAI Playground (or keep going to the next cells and use the API) to try the model!"
      ],
      "metadata": {
        "id": "XUSX5QzmZMTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=10)"
      ],
      "metadata": {
        "id": "45DJZ7hHaBx0",
        "outputId": "7d2e5bf8-af71-4df3-f6fe-dbeee69a758d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-vPURRP4giktfMGMZEnQ0Uw2X', created_at=1699781307, level='info', message='The job has successfully completed', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-zU3u58jpgN0mTI9Q9QenhcCm', created_at=1699781304, level='info', message='New fine-tuned model created: ft:gpt-3.5-turbo-1106:personal::8K13Q84y', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-B7CMJ8wtQ0hPj5cgGqyX9DAs', created_at=1699781258, level='info', message='Step 141/150: training loss=0.00', object='fine_tuning.job.event', data={'step': 141, 'train_loss': 0.0, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-8YM2D4B53VsyGPKLKae7NHPd', created_at=1699781212, level='info', message='Step 131/150: training loss=0.00', object='fine_tuning.job.event', data={'step': 131, 'train_loss': 6.35782896551973e-07, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-JGqsqxTx0X4irLqOuOdbvhZm', created_at=1699781164, level='info', message='Step 121/150: training loss=0.00', object='fine_tuning.job.event', data={'step': 121, 'train_loss': 6.35782896551973e-07, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-YGLWYbhrChmzRny0ow7oByl3', created_at=1699781118, level='info', message='Step 111/150: training loss=0.00', object='fine_tuning.job.event', data={'step': 111, 'train_loss': 6.35782896551973e-07, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-vhpjNWjkgXLPHlnoqsbNnNlY', created_at=1699781072, level='info', message='Step 101/150: training loss=0.00', object='fine_tuning.job.event', data={'step': 101, 'train_loss': 6.35782896551973e-07, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-3FvL2jej86v6fzDEFVVF7DIO', created_at=1699781025, level='info', message='Step 91/150: training loss=0.00', object='fine_tuning.job.event', data={'step': 91, 'train_loss': 6.35782896551973e-07, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-LUtIGVPJs3iqRBVBkSnGzzfu', created_at=1699780979, level='info', message='Step 81/150: training loss=0.00', object='fine_tuning.job.event', data={'step': 81, 'train_loss': 6.35782896551973e-07, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-cu6vDZton0c2rzvz1FEAi3DG', created_at=1699780933, level='info', message='Step 71/150: training loss=0.00', object='fine_tuning.job.event', data={'step': 71, 'train_loss': 6.35782896551973e-07, 'train_mean_token_accuracy': 1.0}, type='metrics')], object='list', has_more=True)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Once your model is trained, run the next cell to grab the fine-tuned model name."
      ],
      "metadata": {
        "id": "91ihW2O27Phl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_pre_object = client.fine_tuning.jobs.retrieve(job_id)\n",
        "model_name = model_name_pre_object.fine_tuned_model\n",
        "print(model_name)"
      ],
      "metadata": {
        "id": "eWBRBPh8aEzH",
        "outputId": "c2f58c4c-9805-43fb-89e2-b7b8d814f4df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ft:gpt-3.5-turbo-1106:personal::8K13Q84y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try it out!"
      ],
      "metadata": {
        "id": "2OmZLoBX7oQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message,\n",
        "      },\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": df['prompt'].sample().values[0],\n",
        "      }\n",
        "    ],\n",
        ")\n",
        "\n",
        "response.choices[0].message.content"
      ],
      "metadata": {
        "id": "uxbrmzc5dMuC",
        "outputId": "a3e63c5f-0ea2-446b-8456-6b78ef3b23a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}